{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# ETHZ: 227-0966-00L\n",
    "# Quantitative Big Imaging\n",
    "# March 5, 2020\n",
    "\n",
    "## Ground Truth: Building and Augmenting Datasets\n",
    "\n",
    "### Anders Kaestner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Today's lecture\n",
    "\n",
    "## Motivation\n",
    "\n",
    "## Creating Datasets\n",
    "- Famous Datasets\n",
    "- Types of Datasets\n",
    "- What makes a good dataet?\n",
    "- Building your own \n",
    " - \"scrape, mine, move, annotate, review, and preprocess\" - Kathy Scott\n",
    " - tools to use\n",
    " - simulation\n",
    "\n",
    "## Augmentation\n",
    "- How can you artifically increase the size of your dataset?\n",
    "- What are the limits of these increases\n",
    "\n",
    "## Baselines\n",
    "- What is a baseline?\n",
    "- Nearest Neighbor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# References\n",
    "\n",
    "- Revisiting **Unreasonable Effectiveness of Data** in Deep Learning Era: https://arxiv.org/abs/1707.02968\n",
    "- Building Datasets\n",
    "    - Python Machine Learning 2nd Edition by Sebastian Raschka, Packt Publishing Ltd. 2017\n",
    "     - Chapter 2: Building Good Datasets: https://github.com/rasbt/python-machine-learning-book-2nd-edition/blob/master/code/ch04/ch04.ipynb\n",
    "    - A Standardised Approach for Preparing Imaging Data for Machine Learning Tasks in Radiology https://link.springer.com/chapter/10.1007/978-3-319-94878-2_6\n",
    "- Creating Datasets / Crowdsourcing\n",
    " - Mindcontrol: A web application for brain segmentation quality control: https://www.sciencedirect.com/science/article/pii/S1053811917302707\n",
    " - Combining citizen science and deep learning to amplify expertise in neuroimaging: https://www.biorxiv.org/content/10.1101/363382v1.abstract\n",
    " \n",
    "- Augmentation\n",
    " - https://github.com/aleju/imgaug\n",
    " - https://github.com/mdbloice/Augmentor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Motivation\n",
    "Most of you taking this class are rightfully excited to learn about new tools and algorithms to analyzing your data. \n",
    "\n",
    "This lecture is a bit of an anomaly and perhaps disappointment because it doesn't cover any algorithms, or tools.\n",
    "- So you might ask, why are we spending so much time on datasets?\n",
    "- You already collected data (sometimes lots of it) that is why you took this class?!\n",
    "\n",
    "... let's see what some other people say"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Sean Taylor (Research Scientist at Facebook)](../common/figures/data_tweet.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Andrej Karpathy (Director of AI at Tesla)\n",
    "<img src=\"../common/figures/karpathy_slide.jpg\" style=\"height:600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Kathy Scott (Image Analytics Lead at Planet Labs)\n",
    "\n",
    "<img src=\"../common/figures/kathy_tweet.png\" style=\"height:700px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Data is important\n",
    "It probably [isn't the _new_ oil](https://www.forbes.com/sites/bernardmarr/2018/03/05/heres-why-data-is-not-the-new-oil/), but it forms an essential component for building modern tools today.\n",
    "- Testing good algorithms *requires* good data\n",
    " - If you don't know what to expect how do you know your algorithm worked?\n",
    " - If you have dozens of edge cases how can you make sure it works on each one?\n",
    " - If a new algorithm is developed every few hours, how can you be confident they actually work better (facebook's site has a new version multiple times per day and their app every other day)\n",
    "\n",
    "- For machine learning, even building requires good data\n",
    " - If you count cells maybe you can write your own algorithm,\n",
    " - but if you are trying to detect subtle changes in cell structure that indicate cancer you probably can't write a list of simple mathematical rules yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Data is reusable\n",
    "\n",
    "- Well organized and structured data is very easy to reuse. \n",
    "- Another project can easily combine your data with their data in order to get even better results.\n",
    "\n",
    "\n",
    "\n",
    "- Algorithms are often messy, complicated, poorly written, ... (especially so if written by students trying to graduate on time)\n",
    "\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<center>Data recycling saves time and improves performance</center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Famous Datasets\n",
    "The primary success of datasets has been shown through the most famous datasets collected. \n",
    "\n",
    "Here I show\n",
    "- Two of the most famous general datasets \n",
    "    - MNIST Digits\n",
    "    - ImageNET\n",
    "- and one of the most famous medical datasets.\n",
    "    - BRATS\n",
    "\n",
    "The famous datasets are important for basic network training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## [MNIST Digits](http://yann.lecun.com/exdb/mnist/)\n",
    "\n",
    "Modified NIST (National Institute of Standards and Technology) created a list of handwritten digits\n",
    " \n",
    " ![Digits](https://raw.githubusercontent.com/yashk2810/yashk2810.github.io/master/images/mnist.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## [ImageNet](http://www.image-net.org)\n",
    " - ImageNet is an image database organized according to the WordNet hierarchy (currently only the nouns), in which each node of the hierarchy is depicted by hundreds and thousands of images.\n",
    " - 1000 different categories and >1M images.\n",
    " - Not just dog/cat, but wolf vs german shepard, \n",
    " \n",
    "![Performance](https://cdn-images-1.medium.com/max/1600/1*DBXf6dzNB78QPHGDofHA4Q.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## [BRATS](http://braintumorsegmentation.org) \n",
    "Segmenting Tumors in Multimodal MRI Brain Images.\n",
    "\n",
    " <img src=\"https://www.med.upenn.edu/sbia/assets/user-content/BRATS_tasks.png\" style=\"height:600px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## What story did these datasets tell?\n",
    "Each of these datasets is very different from images with fewer than 1000 pixels to images with more than 100MPx, but what they have in common is how their analysis has changed.\n",
    "### Hand-crafted features\n",
    "All of these datasets used to be analyzed by domain experts with hand-crafted features. \n",
    "- A handwriting expert using graph topology to assign images to digits\n",
    "- A computer vision expert using gradients common in faces to identify people in ImageNet\n",
    "- A biomedical engineer using knowledge of different modalities to fuse them together and cluster healthy and tumorous tissue\n",
    "\n",
    "### Machine Learning / Deep Learning\n",
    "Starting in the early 2010s, the approaches of deep learning began to improve and become more computationally efficient. With these techniques groups with __absolutely no domain knowledge__ could begin building algorithms and winning contests based on these datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## So Deep Learning always wins? \n",
    "No, that isn't the point of this lecture. \n",
    "\n",
    "Even if you aren't using deep learning the point of these stories is having \n",
    "- well-labeled, \n",
    "- structured, \n",
    "- and organized datasets \n",
    "\n",
    "makes your problem *a lot more accessible* for other groups and enables a variety of different approaches to be tried. \n",
    "\n",
    "Ultimately it enables better solutions to be made and you to be confident that the solutions are in fact better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Other Datasets\n",
    "- Grand-Challenge.org a large number of challenges in the biomedical area\n",
    "- [Kaggle Datasets](https://www.kaggle.com/datasets)\n",
    "- Google Dataset Search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What makes a good dataset?\n",
    "\n",
    "- Lots of images\n",
    " - Small datasets can be useful but here the bigger the better\n",
    " - Particularly if you have complicated problems and/or very subtle differences (ie a lung tumor looks mostly like normal lung tissue but it is in a place it shouldn't be)\n",
    "- Lots of diversity\n",
    " - Is it what data 'in the wild' really looks like?\n",
    " - Lots of different scanners/reconstruction algorithms, noise, illumination types, rotation, colors, ...\n",
    " - Many examples from different categories (if you only have one male with breast cancer it will be hard to generalize exactly what that looks like)\n",
    "- Meaningful labels\n",
    " - Clear task or question\n",
    " - Unambiguous (would multiple different labelers come to the same conclusion)\n",
    " - Able to be derived from the image alone (a label that someone cannot afford insurance is interesting but it would be nearly impossible to determine that from an X-ray of their lungs)\n",
    " - Quantiative!\n",
    " - Non-obvious (a label saying an image is bright is not a helpful label because you could look at the histogram and say that)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Purpose of different types of Datasets\n",
    "- Classification\n",
    "- Regression\n",
    "- Segmentation\n",
    "- Detection\n",
    "- Other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Classification\n",
    "- Taking an image and putting it into a category\n",
    "- Each image should have exactly one category\n",
    "- The categories should be non-ordered\n",
    "- Example: \n",
    " - Cat vs Dog\n",
    " - Cancer vs Healthy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Classification example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import mnist\n",
    "from skimage.util import montage as montage2d\n",
    "%matplotlib inline\n",
    "(img, label), _ = mnist.load_data()\n",
    "fig, m_axs = plt.subplots(5, 5, figsize=(9, 9))\n",
    "for c_ax, c_img, c_label in zip(m_axs.flatten(), img, label):\n",
    "    c_ax.imshow(c_img, cmap='gray')\n",
    "    c_ax.set_title(c_label)\n",
    "    c_ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Regression\n",
    "- Taking an image and predicting one (or more) decimal values\n",
    "- Examples: \n",
    " - Value of a house from the picture taken by owner\n",
    " - Risk of hurricane from satellite image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Regression example [Age from X-Rays](https://www.kaggle.com/kmader/rsna-bone-age) \n",
    "<center>\n",
    "<img src=\"../common/figures/bone_age.png\" style=\"height=500px\">\n",
    "</center>\n",
    "\n",
    "[More details](https://www.kaggle.com/kmader/attention-on-pretrained-vgg16-for-bone-age)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Segmentation\n",
    "- Taking an image and predicting one (or more) values for each pixel\n",
    "- Every pixel needs a label (and a pixel cannot have multiple labels)\n",
    "- Typically limited to a few (<20) different types of objects\n",
    "- Examples:\n",
    " - Where a tumor is from an image of the lungs\n",
    " - Where streets are from satellite images of a neighborhood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Segmentation example: Nuclei in Microscope Images\n",
    "\n",
    "<table>\n",
    "     <tr>\n",
    "        <th>\n",
    "            <center>Image</center>\n",
    "        </th>\n",
    "        <th>\n",
    "            <center>Labelled</center>\n",
    "        </th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src=\"../common/figures/dsb_sample/slide.png\" style=\"height:600px\">\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src=\"../common/figures/dsb_sample/labels.png\" style=\"height:600px\">\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "[More details on Kaggle](https://www.kaggle.com/c/data-science-bowl-2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Detection\n",
    " - Taking an image and predicting where and which type of objects appear\n",
    " - Generally bounding box rather then specific pixels\n",
    " - Multiple objects can overlap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Detection example: Opaque Regions in X-Rays\n",
    "\n",
    "![Opacities](../common/figures/lung_opacity.png)\n",
    "\n",
    "[More details on Kaggle](https://www.kaggle.com/c/rsna-pneumonia-detection-challenge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Other\n",
    " - Unlimited possibilities [here](https://junyanz.github.io/CycleGAN/)\n",
    " - Horses to Zebras \n",
    "\n",
    "## Image Enhancement \n",
    "  - Denoising [Learning to See in the Dark](http://cchen156.web.engr.illinois.edu/SID.html)\n",
    "  - [Super-resolution](https://data.vision.ee.ethz.ch/cvl/DIV2K/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Building your own data sets\n",
    "\n",
    "- Very time consuming\n",
    "- Not a lot of great tools\n",
    "- Very problem specific"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Code-free\n",
    "\n",
    "### Classification\n",
    "- Organize images into folders\n",
    "\n",
    "### Regression\n",
    "- Create an excel file (first column image name, next columns to regress)\n",
    "\n",
    "### Segmentation / Object Detection\n",
    "- Take [FIJI](http://fiji.sc/) and manually draw region to be identified and save it as a grayscale image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Software for data labelling\n",
    "### Free tools\n",
    " - Classification / Segmentation: https://github.com/Labelbox/Labelbox\n",
    " - Classification/ Object Detection: http://labelme.csail.mit.edu/Release3.0/\n",
    " - Classification: https://github.com/janfreyberg/superintendent: https://www.youtube.com/watch?v=fMg0mPYiEx0\n",
    " - Classification/ Detection: https://github.com/chestrays/jupyanno: https://www.youtube.com/watch?v=XDIJU5Beg_w\n",
    " - Classification (Tinder for Brain MRI): https://braindr.us/#/\n",
    " \n",
    "### Commercial Approaches\n",
    " - https://www.figure-eight.com/\n",
    " - MightyAI / Spare5: https://mighty.ai/ https://app.spare5.com/fives/sign_in\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Simulations\n",
    "Another way to enhance or expand your dataset is to use simulations\n",
    "- already incorporate realistic data (game engines, 3D rendering, physics models)\n",
    "- 100% accurate ground truth (original models)\n",
    "- unlimited, risk-free playability (driving cars in the world is more dangerous)\n",
    "\n",
    "#### Examples\n",
    "\n",
    "- [P. Fuchs et al. - Generating Meaningful Synthetic Ground Truth for\n",
    "Pore Detection in Cast Aluminum Parts, iCT 2019, Padova](https://pdfs.semanticscholar.org/30a1/ba9142b9c3b755da2bff7d93d704494fdaed.pdf)\n",
    "- https://download.visinf.tu-darmstadt.de/data/from_games/\n",
    "- https://pythonprogramming.net/self-driving-car-neural-network-training-data-python-plays-gta-v/\n",
    "- https://towardsdatascience.com/learning-from-simulated-data-ff4be63ac89c\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Dataset Problems\n",
    "Some of the issues which can come up with datasets are \n",
    "- imbalance\n",
    "- too few examples\n",
    "- too homogenous \n",
    "- and other possible problems\n",
    "\n",
    "These lead to problems with the algorithms built on top of them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Bias\n",
    "\n",
    "![image.png](https://pbs.twimg.com/media/CIoW7wBWoAEqQRP.png:large)\n",
    "\n",
    "<blockquote class=\"twitter-tweet\" data-lang=\"en\"><p lang=\"en\" dir=\"ltr\">Google Photos, y&#39;all *** up. My friend&#39;s not a gorilla. <a href=\"http://t.co/SMkMCsNVX4\">pic.twitter.com/SMkMCsNVX4</a></p>&mdash; I post from https://v2.jacky.wtf. ðŸ†“ != safe. (@jackyalcine) <a href=\"https://twitter.com/jackyalcine/status/615329515909156865?ref_src=twsrc%5Etfw\">June 29, 2015</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script> \n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "[Solution was to remove Gorilla from the category](https://www.theverge.com/2018/1/12/16882408/google-racist-gorillas-photo-recognition-algorithm-ai)\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Better solution would be to have training sets with more diverse people\n",
    "\n",
    "<img src=\"../common/figures/celeb_dataset.png\" style=\"height:700px\">\n",
    "\n",
    "- [IBM Diverse Face Dataset](https://www.research.ibm.com/artificial-intelligence/trusted-ai/diversity-in-faces/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Example of image data and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import mnist\n",
    "from skimage.util import montage as montage2d\n",
    "%matplotlib inline\n",
    "(img, label), _ = mnist.load_data()\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
    "d_subset = np.where(np.in1d(label, [1, 2, 3]))[0]\n",
    "ax1.imshow(montage2d(img[d_subset[:64]]), cmap='gray')\n",
    "ax1.set_title('Images')\n",
    "ax1.axis('off')\n",
    "ax2.hist(label[d_subset[:64]], np.arange(11))\n",
    "ax2.set_title('Digit Distribution');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Augmentation\n",
    "- Most groups have too little well-labeled data and labeling new examples can be very expensive. \n",
    "- Additionally there might not be very many cases of specific classes. \n",
    "- In medicine this is particularly problematic, because some diseases might only happen a few times in a given hospital and you still want to be able to recognize the disease and not that particular person."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Solutions to limited data\n",
    "## Transformation\n",
    "- Shift\n",
    "- Zoom\n",
    "- Rotation\n",
    "- Intensity\n",
    " - Normalization\n",
    " - Scaling\n",
    "- Color\n",
    "- Shear\n",
    "\n",
    "## Further modifications\n",
    "- Add noise\n",
    "- Blurring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Some augmentation examples\n",
    "<figure>\n",
    "<img src=\"../common/figures/Augmentations.svg\" style=\"height:500px\">\n",
    "<figcaption>\n",
    "    \n",
    "Retial images from [DRIVE](https://drive.grand-challenge.org/DRIVE/) prepared by Gian Guido Parenza.\n",
    "    \n",
    "</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Limitations of augmentation\n",
    "\n",
    "- What transformations are normal in the images?\n",
    " - CT images usually do not get flipped (the head is always on the top)\n",
    " - The values in CT images have a physical meaning (Hounsfield unit), <br /> $\\rightarrow$ scaling them changes the image\n",
    " \n",
    " \n",
    "- How much distortion is too much? \n",
    " - Can you still recognize the features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Manual for the ImageDataGenerator\n",
    "```\n",
    "ImageDataGenerator(\n",
    "    ['featurewise_center=False', 'samplewise_center=False', 'featurewise_std_normalization=False', 'samplewise_std_normalization=False', 'zca_whitening=False', 'zca_epsilon=1e-06', 'rotation_range=0.0', 'width_shift_range=0.0', 'height_shift_range=0.0', 'shear_range=0.0', 'zoom_range=0.0', 'channel_shift_range=0.0', \"fill_mode='nearest'\", 'cval=0.0', 'horizontal_flip=False', 'vertical_flip=False', 'rescale=None', 'preprocessing_function=None', 'data_format=None'],\n",
    ")\n",
    "Docstring:     \n",
    "Generate minibatches of image data with real-time data augmentation.\n",
    "\n",
    "# Arguments\n",
    "    featurewise_center: set input mean to 0 over the dataset.\n",
    "    samplewise_center: set each sample mean to 0.\n",
    "    featurewise_std_normalization: divide inputs by std of the dataset.\n",
    "    samplewise_std_normalization: divide each input by its std.\n",
    "    zca_whitening: apply ZCA whitening.\n",
    "    zca_epsilon: epsilon for ZCA whitening. Default is 1e-6.\n",
    "    rotation_range: degrees (0 to 180).\n",
    "    width_shift_range: fraction of total width, if < 1, or pixels if >= 1.\n",
    "    height_shift_range: fraction of total height, if < 1, or pixels if >= 1.\n",
    "    shear_range: shear intensity (shear angle in degrees).\n",
    "    zoom_range: amount of zoom. if scalar z, zoom will be randomly picked\n",
    "        in the range [1-z, 1+z]. A sequence of two can be passed instead\n",
    "        to select this range.\n",
    "    channel_shift_range: shift range for each channel.\n",
    "    fill_mode: points outside the boundaries are filled according to the\n",
    "        given mode ('constant', 'nearest', 'reflect' or 'wrap'). Default\n",
    "        is 'nearest'.\n",
    "        Points outside the boundaries of the input are filled according to the given mode:\n",
    "            'constant': kkkkkkkk|abcd|kkkkkkkk (cval=k)\n",
    "            'nearest':  aaaaaaaa|abcd|dddddddd\n",
    "            'reflect':  abcddcba|abcd|dcbaabcd\n",
    "            'wrap':  abcdabcd|abcd|abcdabcd\n",
    "    cval: value used for points outside the boundaries when fill_mode is\n",
    "        'constant'. Default is 0.\n",
    "    horizontal_flip: whether to randomly flip images horizontally.\n",
    "    vertical_flip: whether to randomly flip images vertically.\n",
    "    rescale: rescaling factor. If None or 0, no rescaling is applied,\n",
    "        otherwise we multiply the data by the value provided. This is\n",
    "        applied after the `preprocessing_function` (if any provided)\n",
    "        but before any other transformation.\n",
    "    preprocessing_function: function that will be implied on each input.\n",
    "        The function will run before any other modification on it.\n",
    "        The function should take one argument:\n",
    "        one image (Numpy tensor with rank 3),\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "img_aug = ImageDataGenerator(\n",
    "    featurewise_center=False,\n",
    "    samplewise_center=False,\n",
    "    zca_whitening=False,\n",
    "    zca_epsilon=1e-06,\n",
    "    rotation_range=30.0,\n",
    "    width_shift_range=0.25,\n",
    "    height_shift_range=0.25,\n",
    "    shear_range=0.25,\n",
    "    zoom_range=0.5,\n",
    "    fill_mode='nearest',\n",
    "    horizontal_flip=False,\n",
    "    vertical_flip=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## MNIST\n",
    "Even something as simple as labeling digits can be very time consuming (maybe 1-2 per second)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import mnist\n",
    "%matplotlib inline\n",
    "(img, label), _ = mnist.load_data()\n",
    "img = np.expand_dims(img, -1)\n",
    "fig, m_axs = plt.subplots(4, 10, figsize=(14, 9))\n",
    "# setup augmentation\n",
    "img_aug.fit(img)\n",
    "real_aug = img_aug.flow(img[:10], label[:10], shuffle=False)\n",
    "for c_axs, do_augmentation in zip(m_axs, [False, True, True, True]):\n",
    "    if do_augmentation:\n",
    "        img_batch, label_batch = next(real_aug)\n",
    "    else:\n",
    "        img_batch, label_batch = img, label\n",
    "    for c_ax, c_img, c_label in zip(c_axs, img_batch, label_batch):\n",
    "        c_ax.imshow(c_img[:, :, 0], cmap='gray', vmin=0, vmax=255)\n",
    "        c_ax.set_title('{}\\n{}'.format(\n",
    "            c_label, 'aug' if do_augmentation else ''))\n",
    "        c_ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# [CIFAR10](https://www.cs.toronto.edu/~kriz/cifar.html)\n",
    "We can use a more exciting dataset to try some of the other features in augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10\n",
    "(img, label), _ = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_aug = ImageDataGenerator(\n",
    "    featurewise_center=True,\n",
    "    samplewise_center=False,\n",
    "    zca_whitening=False,\n",
    "    zca_epsilon=1e-06,\n",
    "    rotation_range=30.0,\n",
    "    width_shift_range=0.25,\n",
    "    height_shift_range=0.25,\n",
    "    channel_shift_range=0.25,\n",
    "    shear_range=0.25,\n",
    "    zoom_range=1,\n",
    "    fill_mode='reflect',\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import mnist\n",
    "%matplotlib inline\n",
    "fig, m_axs = plt.subplots(4, 10, figsize=(16, 12))\n",
    "# setup augmentation\n",
    "img_aug.fit(img)\n",
    "real_aug = img_aug.flow(img[:10], label[:10], shuffle=False)\n",
    "for c_axs, do_augmentation in zip(m_axs, [False, True, True, True]):\n",
    "    if do_augmentation:\n",
    "        img_batch, label_batch = next(real_aug)\n",
    "        img_batch -= img_batch.min()\n",
    "        img_batch = np.clip(img_batch/img_batch.max() *\n",
    "                            255, 0, 255).astype('uint8')\n",
    "    else:\n",
    "        img_batch, label_batch = img, label\n",
    "    for c_ax, c_img, c_label in zip(c_axs, img_batch, label_batch):\n",
    "        c_ax.imshow(c_img)\n",
    "        c_ax.set_title('{}\\n{}'.format(\n",
    "            c_label[0], 'aug' if do_augmentation else ''))\n",
    "        c_ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Baselines\n",
    "- A baseline is a simple, easily implemented and understood model that illustrates the problem and the 'worst-case scenario' for a model that learns nothing (some models will do worse, but these are especially useless).\n",
    "- Why is this important?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Baseline model example\n",
    "I have a a model that is >99% accurate for predicting breast cancer\n",
    "\n",
    "$$ \\textrm{DoIHaveBreastCancer}(\\textrm{Age}, \\textrm{Weight}, \\textrm{Race}) = \\textrm{No!} $$\n",
    "\n",
    "<div class=\"alert alert-box alert-danger\">\n",
    "Breast Cancer incidence is $\\approx$ 89 of 100,000 women (0.09%) so always saying <b>no</b> has an accuracy of <b>99.91%</b>.\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "dc = DummyClassifier(strategy='most_frequent')\n",
    "dc.fit([0, 1, 2, 3], \n",
    "       ['Healthy', 'Healthy', 'Healthy', 'Cancer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "dc.predict([0]), dc.predict([1]), dc.predict([3]), dc.predict([100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import mnist\n",
    "from skimage.util import montage as montage2d\n",
    "%matplotlib inline\n",
    "(img, label), _ = mnist.load_data()\n",
    "fig, m_axs = plt.subplots(5, 5, figsize=(12, 12))\n",
    "m_axs[0, 0].hist(label[:24], np.arange(11))\n",
    "m_axs[0, 0].set_title('Digit Distribution')\n",
    "for i, c_ax in enumerate(m_axs.flatten()[1:]):\n",
    "    c_ax.imshow(img[i], cmap='gray')\n",
    "    c_ax.set_title(label[i])\n",
    "    c_ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Let's train the model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "dc = DummyClassifier(strategy='most_frequent')\n",
    "dc.fit(img[:24], label[:24])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## A basic test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "dc.predict(img[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "#### ... why are all = 1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.hist(label[:24], np.arange(11)); plt.title('Frequency of numbers in the training data');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Test on the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "fig, m_axs = plt.subplots(4, 6, figsize=(12, 12))\n",
    "for i, c_ax in enumerate(m_axs.flatten()):\n",
    "    c_ax.imshow(img[i], cmap='gray')\n",
    "    c_ax.set_title('{}\\nPredicted: {}'.format(label[i], dc.predict(img[i])[0]))\n",
    "    c_ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Nearest Neighbor\n",
    "This isn't a machine learning class and so we won't dive deeply into other methods, but nearest neighbor is often a very good baseline (that is also very easy to understand). You basically take the element from the original set that is closest to the image you show. \n",
    "\n",
    "<img src=\"../common/figures/Russ_fig12_58.png\" style=\"height:300px\">\n",
    "\n",
    "[Figure from J. Russ, Image Processing Handbook](https://www.crcpress.com/The-Image-Processing-Handbook/Russ-Neal/p/book/9781138747494)    \n",
    "\n",
    "You can make the method more robust by using more than one nearest neighbor (hence K nearest neighbors), but that we will cover in the supervised methods lecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Let's load the data again..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import mnist\n",
    "from skimage.util import montage as montage2d\n",
    "%matplotlib inline\n",
    "(img, label), _ = mnist.load_data()\n",
    "fig, m_axs = plt.subplots(5, 5, figsize=(12, 12))\n",
    "m_axs[0, 0].hist(label[:24], np.arange(11))\n",
    "m_axs[0, 0].set_title('Digit Distribution')\n",
    "for i, c_ax in enumerate(m_axs.flatten()[1:]):\n",
    "    c_ax.imshow(img[i], cmap='gray')\n",
    "    c_ax.set_title(label[i])\n",
    "    c_ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "neigh_class = KNeighborsClassifier(n_neighbors=1)\n",
    "neigh_class.fit(img[:24].reshape((24, -1)), label[:24])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# predict on a few images\n",
    "neigh_class.predict(img[0:10].reshape((10, -1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fig, m_axs = plt.subplots(4, 6, figsize=(12, 12))\n",
    "for i, c_ax in enumerate(m_axs.flatten()):\n",
    "    c_ax.imshow(img[i], cmap='gray')\n",
    "    c_ax.set_title('{}\\nPredicted: {}'.format(label[i], \n",
    "                                              neigh_class.predict(img[i].reshape((1, -1)))[0]))\n",
    "    c_ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 100% for a baseline\n",
    "Wow the model works really really well, it got every example perfectly. What we did here (a common mistake) was evaluate on the same data we 'trained' on which means the model just correctly recalled each example, if we try it on new images we can see the performance drop but still a reasonable result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fig, m_axs = plt.subplots(4, 6, figsize=(12, 12))\n",
    "for i, c_ax in enumerate(m_axs.flatten(), 25):\n",
    "    c_ax.imshow(img[i], cmap='gray')\n",
    "    c_ax.set_title('{}\\nPredicted: {}'.format(label[i], \n",
    "                                              neigh_class.predict(img[i].reshape((1, -1)))[0]))\n",
    "    c_ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## How good is good?\n",
    "We will cover more tools later in the class but now we will show the accuracy and the confusion matrix for our simple baseline model to evaluate how well it worked\n",
    "\n",
    "### Confusion Matrix\n",
    "We show which cases were most frequently confused "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "def print_confusion_matrix(confusion_matrix, class_names, figsize = (10,7), fontsize=14):\n",
    "    \"\"\"Prints a confusion matrix, as returned by sklearn.metrics.confusion_matrix, as a heatmap.\n",
    "    \n",
    "    Stolen from: https://gist.github.com/shaypal5/94c53d765083101efc0240d776a23823\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    confusion_matrix: numpy.ndarray\n",
    "        The numpy.ndarray object returned from a call to sklearn.metrics.confusion_matrix. \n",
    "        Similarly constructed ndarrays can also be used.\n",
    "    class_names: list\n",
    "        An ordered list of class names, in the order they index the given confusion matrix.\n",
    "    figsize: tuple\n",
    "        A 2-long tuple, the first value determining the horizontal size of the ouputted figure,\n",
    "        the second determining the vertical size. Defaults to (10,7).\n",
    "    fontsize: int\n",
    "        Font size for axes labels. Defaults to 14.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    matplotlib.figure.Figure\n",
    "        The resulting confusion matrix figure\n",
    "    \"\"\"\n",
    "    df_cm = pd.DataFrame(\n",
    "        confusion_matrix, index=class_names, columns=class_names, \n",
    "    )\n",
    "    fig, ax1 = plt.subplots(1, 1, figsize=figsize)\n",
    "    try:\n",
    "        heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\")\n",
    "    except ValueError:\n",
    "        raise ValueError(\"Confusion matrix values must be integers.\")\n",
    "    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=fontsize)\n",
    "    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    return ax1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "pred_values = neigh_class.predict(img[24:].reshape((-1, 28*28)))\n",
    "ax1 = print_confusion_matrix(confusion_matrix(label[24:], pred_values), class_names=range(10))\n",
    "ax1.set_title('Accuracy: {:2.2%}'.format(accuracy_score(label[24:], pred_values)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "livereveal": {
   "autolaunch": true,
   "enable_chalkboard": true,
   "footer": "March 5, 2020 - ETH 227-0966-00L: Quantitative Big Imaging/Ground truth",
   "header": "<table width='100%' style='margin: 0px;'><tr><td align='left'><img src='../common/figures/eth_logo_kurz_pos.svg' style='height:30px;'></td><td align='right'><img src='../common/figures/PSI-Logo.svg' style='height:50px;'></td></tr></table>",
   "scroll": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
