{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ETHZ: 227-0966-00L\n",
    "# Quantitative Big Imaging\n",
    "# March 21, 2019\n",
    "\n",
    "## Advanced Segmentation\n",
    "\n",
    "### Anders Kaestner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Literature / Useful References\n",
    "\n",
    "- [Markov Random Fields for Image Processing Lecture](https://www.youtube.com/watch?v=vRN_j2j-CC4)\n",
    "- [Markov Random Fields Chapter](http://www.cise.ufl.edu/~anand/pdf/bookchap.pdf)\n",
    "- [Fuzzy set theory](http://www.academia.edu/4978200/Applications_of_Fuzzy_Set_Theory_and_Fuzzy_Logic_in_Image_Processing)\n",
    "- [Superpixels](http://ivrg.epfl.ch/research/superpixels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Outline\n",
    "\n",
    "- Motivation\n",
    " - Many Samples\n",
    " - Difficult Samples\n",
    " - Training / Learning\n",
    "- Thresholding\n",
    " - Automated Methods\n",
    " - Hysteresis Method\n",
    "\n",
    "*** \n",
    "\n",
    "- Feature Vectors\n",
    " - K-Means Clustering\n",
    " - Superpixels\n",
    " - Probabalistic Models\n",
    "- Working with Segmented Images\n",
    " - Contouring\n",
    "- Beyond\n",
    " - Fuzzy Models\n",
    " - Component Labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " \n",
    "# Previously on QBI\n",
    "\n",
    "- Image acquisition and representations\n",
    "- Enhancement and noise reduction\n",
    "- Understanding models and interpreting histograms\n",
    "- Ground Truth and ROC Curves\n",
    "\n",
    "*** \n",
    "\n",
    "- Choosing a threshold\n",
    " - Examining more complicated, multivariate data sets\n",
    "- Improving segementation with morphological operations\n",
    " - Filling holes\n",
    " - Connecting objects\n",
    " - Removing Noise\n",
    "- Partial Volume Effect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Where segmentation fails: Inconsistent Illumination\n",
    "\n",
    "With inconsistent or every changing illumination it may not be possible to apply the same threshold to every image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from skimage.io import imread\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "cell_img = imread(\"../common/figures/Cell_Colony.jpg\")/255.0\n",
    "np.random.seed(2018)\n",
    "m_cell_imgs = [cell_img+k for k in np.random.uniform(-0.25, 0.25, size = 8)]\n",
    "fig, m_axs = plt.subplots(3, 3, figsize = (12, 12), dpi = 72)\n",
    "ax1 = m_axs[0,0]\n",
    "for i, (c_ax, c_img) in enumerate(zip(m_axs.flatten()[1:], m_cell_imgs)):\n",
    "    ax1.hist(c_img.ravel(), np.linspace(0, 1, 20), \n",
    "             label = '{}'.format(i), alpha = 0.5, histtype = 'step')\n",
    "    c_ax.imshow(c_img, cmap = 'bone', vmin = 0, vmax = 1)\n",
    "    c_ax.axis('off')\n",
    "ax1.set_yscale('log', nonposy = 'clip')\n",
    "ax1.set_ylabel('Pixel Count (Log)')\n",
    "ax1.set_xlabel('Intensity')\n",
    "ax1.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A constant threshold of 0.6 for the different illuminations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fig, m_axs = plt.subplots(len(m_cell_imgs), 2, \n",
    "                          figsize = (4*2, 4*len(m_cell_imgs)), \n",
    "                          dpi = 72)\n",
    "THRESH_VAL = 0.6\n",
    "for i, ((ax1, ax2), c_img) in enumerate(zip(m_axs, \n",
    "                                   m_cell_imgs)):\n",
    "    ax1.hist(c_img.ravel()[c_img.ravel()>THRESH_VAL], \n",
    "             np.linspace(0, 1, 100), \n",
    "             label = 'Above Threshold', \n",
    "             alpha = 0.5)\n",
    "    ax1.hist(c_img.ravel()[c_img.ravel()<THRESH_VAL], \n",
    "             np.linspace(0, 1, 100), \n",
    "             label = 'Below Threshold', \n",
    "             alpha = 0.5)\n",
    "    ax1.set_yscale('log', nonposy = 'clip')\n",
    "    ax1.set_ylabel('Pixel Count (Log)')\n",
    "    ax1.set_xlabel('Intensity')\n",
    "    ax1.legend()\n",
    "    ax2.imshow(c_img<THRESH_VAL, \n",
    "               cmap = 'bone', \n",
    "               vmin = 0, vmax = 1)\n",
    "    ax2.set_title('Volume Fraction (%2.2f%%)' % (100*np.mean(c_img.ravel()<THRESH_VAL)))\n",
    "    ax2.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Where segmentation fails: Canaliculi\n",
    "\n",
    "![Bone Slice](ext-figures/bonegfiltslice.png)\n",
    "\n",
    "### Here is a bone slice\n",
    "\n",
    "1. Find the larger cellular structures (osteocyte lacunae)\n",
    "1. Find the small channels which connect them together\n",
    "\n",
    "***\n",
    "\n",
    "### The first task \n",
    "is easy using a threshold and size criteria (we know how big the cells should be)\n",
    "\n",
    "### The second \n",
    "is much more difficult because the small channels having radii on the same order of the pixel size are obscured by partial volume effects and noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Where segmentation fails: Brain Cortex\n",
    "\n",
    "![Cortex Image](ext-figures/cortex_mask.png)\n",
    "\n",
    "- The cortex is barely visible to the human eye\n",
    "- Tiny structures hint at where cortex is located\n",
    "\n",
    "*** \n",
    "\n",
    "- A simple threshold is insufficient to finding the cortical structures\n",
    "- Other filtering techniques are unlikely to magicially fix this problem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from skimage.io import imread\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "cortex_img = imread(\"ext-figures/cortex.png\")\n",
    "np.random.seed(2018)\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, \n",
    "                                    figsize = (12, 4), dpi = 72)\n",
    "ax1.imshow(cortex_img, cmap = 'bone')\n",
    "thresh_vals = np.linspace(cortex_img.min(), cortex_img.max(), 4+2)[:-1]\n",
    "out_img = np.zeros_like(cortex_img)\n",
    "for i, (t_start, t_end) in enumerate(zip(thresh_vals, thresh_vals[1:])):\n",
    "    thresh_reg = (cortex_img>t_start) & (cortex_img<t_end)\n",
    "    ax2.hist(cortex_img.ravel()[thresh_reg.ravel()])\n",
    "    out_img[thresh_reg] = i\n",
    "ax3.imshow(out_img, cmap = 'gist_earth');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Automated Threshold Selection\n",
    "\n",
    "![Many possible automated techniques](ext-figures/automaticthresh.png)\n",
    "\n",
    "***\n",
    "\n",
    "Given that applying a threshold is such a common and signficant step, there have been many tools developed to automatically (unsupervised) perform it. A particularly important step in setups where images are rarely consistent such as outdoor imaging which has varying lighting (sun, clouds). The methods are based on several basic principles. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Automated Methods\n",
    "\n",
    "\n",
    "### Histogram-based methods\n",
    "Just like we visually inspect a histogram an algorithm can examine the histogram and find local minimums between two peaks, maximum / minimum entropy and other factors\n",
    "\n",
    "- Otsu, Isodata, Intermodes, etc\n",
    "\n",
    "### Image based Methods\n",
    "These look at the statistics of the thresheld image themselves (like entropy) to estimate the threshold  \n",
    "\n",
    "\n",
    "### Results-based Methods\n",
    "These search for a threshold which delivers the desired results in the final objects. For example if you know you have an image of cells and each cell is between 200-10000 pixels the algorithm runs thresholds until the objects are of the desired size\n",
    "- More specific requirements need to be implemented manually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Histogram Methods\n",
    "\n",
    "\n",
    "Taking a typical image of a bone slice, we can examine the variations in calcification density in the image\n",
    "\n",
    "***\n",
    "\n",
    "We can see in the histogram that there are two peaks, one at 0 (no absorption / air) and one at 0.5 (stronger absorption / bone)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from skimage.io import imread\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "bone_img = imread(\"ext-figures/bonegfiltslice.png\")/255.0\n",
    "np.random.seed(2018)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2,figsize = (12, 4), dpi = 72)\n",
    "ax1.imshow(bone_img, cmap = 'bone')\n",
    "ax1.axis('off')\n",
    "ax2.hist(bone_img.ravel(), bins=256);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Histogram Methods\n",
    "\n",
    "***\n",
    "\n",
    "### Intermodes\n",
    " - Take the point between the two modes (peaks) in the histogram\n",
    " \n",
    "### Otsu\n",
    "\n",
    "Search and minimize intra-class (within) variance\n",
    "$$\\sigma^2_w(t)=\\omega_{bg}(t)\\sigma^2_{bg}(t)+\\omega_{fg}(t)\\sigma^2_{fg}(t)$$\n",
    "\n",
    "### Isodata\n",
    "\n",
    "- $\\textrm{thresh}= \\frac{\\max(img)+\\min(img)}{2}$\n",
    "- _while_ the thresh is changing\n",
    " - $bg = img<\\textrm{thresh}, obj = img>\\textrm{thresh}$\n",
    " - $\\textrm{thresh} = (\\textrm{avg}(bg) + \\textrm{avg}(obj))/2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Try All Thresholds\n",
    "\n",
    "- [opencv2](https://docs.opencv.org/2.4/doc/tutorials/imgproc/threshold/threshold.html)\n",
    "- [scikit-image](http://scikit-image.org/docs/dev/auto_examples/xx_applications/plot_thresholding.html)\n",
    "\n",
    "There are many methods and they can be complicated to implement yourself. Both Fiji and scikit-image offers many of them as built in functions so you can automatically try all of them on your image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from skimage.io import imread\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "bone_img = imread(\"ext-figures/bonegfiltslice.png\")/255.0\n",
    "\n",
    "from skimage.filters import try_all_threshold\n",
    "\n",
    "fig, ax = try_all_threshold(bone_img, figsize=(10, 8), verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Pitfalls\n",
    "\n",
    "While an incredibly useful tool, there are many potential pitfalls to these automated techniques. \n",
    "\n",
    "### Histogram-based\n",
    "\n",
    "These methods are very sensitive to the distribution of pixels in your image and may work really well on images with equal amounts of each phase but work horribly on images which have very high amounts of one phase compared to the others\n",
    "\n",
    "### Image-based\n",
    "\n",
    "These methods are sensitive to noise and a large noise content in the image can change statistics like entropy significantly. \n",
    "\n",
    "### Results-based\n",
    "\n",
    "These methods are inherently biased by the expectations you have. If you want to find objects between 200 and 1000 pixels you will, they just might not be anything meaningful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Realistic Approaches for Dealing with these Shortcomings\n",
    "\n",
    "\n",
    "Imaging science rarely represents the ideal world and will never be 100% perfect. At some point we need to write our master's thesis, defend, or publish a paper. These are approaches for more qualitative assessment we will later cover how to do this a bit more robustly with quantitative approaches\n",
    "\n",
    "### Model-based\n",
    "\n",
    "One approach is to try and simulate everything (including noise) as well as possible and to apply these techniques to many realizations of the same image and qualitatively keep track of how many of the results accurately identify your phase or not. Hint: >95% seems to convince most biologists\n",
    "\n",
    "### Sample-based\n",
    "\n",
    "Apply the methods to each sample and keep track of which threshold was used for each one. Go back and apply each threshold to each sample in the image and keep track of how many of them are correct enough to be used for further study.\n",
    "\n",
    "### Worst-case Scenario\n",
    "\n",
    "Come up with the worst-case scenario (noise, misalignment, etc) and assess how inacceptable the results are. Then try to estimate the quartiles range (75% - 25% of images)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hysteresis Thresholding\n",
    "\n",
    "For some images a single threshold does not work\n",
    "- large structures are very clearly defined\n",
    "- smaller structures are difficult to differentiate (see [partial volume effect](http://bit.ly/1mW7kdP))\n",
    "\n",
    "[ImageJ Source](http://imagejdocu.tudor.lu/doku.php?id=plugin:segmentation:hysteresis_thresholding:start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from skimage.io import imread\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "bone_img = imread(\"ext-figures/bonegfiltslice.png\")\n",
    "np.random.seed(2018)\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, \n",
    "                                    figsize = (12, 4), dpi = 150)\n",
    "cmap = plt.cm.nipy_spectral_r\n",
    "ax1.imshow(bone_img, cmap = 'bone')\n",
    "thresh_vals = [0, 70, 110, 255]\n",
    "out_img = np.zeros_like(bone_img)\n",
    "for i, (t_start, t_end) in enumerate(zip(thresh_vals, thresh_vals[1:])):\n",
    "    thresh_reg = (bone_img>t_start) & (bone_img<t_end)\n",
    "    ax2.hist(bone_img.ravel()[thresh_reg.ravel()], \n",
    "             color = cmap(i/(len(thresh_vals))))\n",
    "    out_img[thresh_reg] = i\n",
    "ax2.set_yscale(\"log\", nonposy='clip')\n",
    "th_ax = ax3.imshow(out_img, \n",
    "                   cmap = cmap, \n",
    "                   vmin = 0, \n",
    "                   vmax = len(thresh_vals))\n",
    "plt.colorbar(th_ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Goldilocks Situation\n",
    "Here we end up with a goldilocks situation (Mama bear and Papa Bear), one is too low and the other is too high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (8, 4), dpi = 300)\n",
    "ax1.imshow(bone_img<thresh_vals[1], cmap = 'bone_r')\n",
    "ax1.set_title('Threshold $<$ %d' % (thresh_vals[1]))\n",
    "ax2.imshow(bone_img<thresh_vals[2], cmap = 'bone_r')\n",
    "ax2.set_title('Threshold $<$ %d' % (thresh_vals[2]));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Baby Bear\n",
    "We can thus follow a process for ending up with a happy medium of the two  \n",
    "\n",
    "## Hysteresis Thresholding: Reducing Pixels\n",
    "\n",
    "Now we apply the following steps. \n",
    "\n",
    "1. Take the first threshold image with the highest (more strict) threshold\n",
    "1. Remove the objects which are not cells (too small) using an opening operation.\n",
    "1. Take a second threshold image with the higher value\n",
    "1. Combine both threshold images\n",
    "1. Keep the _between_ pixels which are connected (by looking again at a neighborhood $\\mathcal{N}$) to the _air_ voxels and ignore the other ones. This goes back to our original supposition that the smaller structures are connected to the larger structures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from skimage.morphology import dilation, opening, disk\n",
    "from collections import OrderedDict\n",
    "step_list = OrderedDict()\n",
    "step_list['Strict Threshold'] = bone_img<thresh_vals[1]\n",
    "step_list['Remove Small Objects'] = opening(step_list['Strict Threshold'], disk(1))\n",
    "step_list['Looser Threshold'] = bone_img<thresh_vals[2]\n",
    "step_list['Both Thresholds'] = 1.0*step_list['Looser Threshold'] + 1.0*step_list['Remove Small Objects']\n",
    "# the tricky part keeping the between images\n",
    "step_list['Connected Thresholds'] = step_list['Remove Small Objects']\n",
    "for i in range(10):\n",
    "    step_list['Connected Thresholds'] = dilation(step_list['Connected Thresholds'] , \n",
    "                                                 disk(1.8)) & step_list['Looser Threshold']\n",
    "\n",
    "\n",
    "fig, ax_steps = plt.subplots(len(step_list), 1,\n",
    "                             figsize = (6, 4*(len(step_list))), \n",
    "                             dpi = 150)\n",
    "\n",
    "for i, (c_ax, (c_title, c_img)) in enumerate(zip(ax_steps.flatten(), step_list.items()),1):\n",
    "    c_ax.imshow(c_img, cmap = 'bone_r' if c_img.max()<=1 else 'viridis')\n",
    "    c_ax.set_title('%d) %s' % (i, c_title))\n",
    "    c_ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "More Complicated Images\n",
    "===\n",
    "As we briefly covered last time, many measurement techniques produce quite rich data. \n",
    "- Digital cameras produce 3 channels of color for each pixel (rather than just one intensity)\n",
    "- MRI produces dozens of pieces of information for every voxel which are used when examining different _contrasts_ in the system.\n",
    "- Raman-shift imaging produces an entire spectrum for each pixel\n",
    "- Coherent diffraction techniques produce 2- (sometimes 3) diffraction patterns for each point.\n",
    "$$ I(x,y) = \\hat{f}(x,y) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Feature Vectors\n",
    "\n",
    "__A pairing between spatial information (position) and some other kind of information (value).__\n",
    "$$ \\vec{x} \\rightarrow \\vec{f} $$\n",
    "\n",
    "We are used to seeing images in a grid format where the position indicates the row and column in the grid and the intensity (absorption, reflection, tip deflection, etc) is shown as a different color. We take an example here of text on a page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from skimage.io import imread\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from skimage.data import page\n",
    "import pandas as pd\n",
    "from skimage.filters import gaussian, median, threshold_triangle\n",
    "page_image = page()\n",
    "just_text = median(page_image, np.ones((2,2)))-255*gaussian(page_image, 20.0)\n",
    "\n",
    "plt.figure(figsize=[15,5])\n",
    "plt.subplot(1,2,1),plt.imshow(page_image, cmap = 'bone'); plt.title('Original');\n",
    "plt.subplot(1,2,2),plt.imshow(just_text, cmap = 'bone'); plt.title('Just text');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Let's create a feature table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "xx, yy = np.meshgrid(np.arange(page_image.shape[1]),\n",
    "           np.arange(page_image.shape[0]))\n",
    "page_table = pd.DataFrame(dict(x = xx.ravel(), \n",
    "                               y = yy.ravel(), \n",
    "                               intensity = page_image.ravel(),\n",
    "                              is_text = just_text.ravel()>0))\n",
    "page_table.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Inspect the features - Intensity vs. IsText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(1,1)\n",
    "for c_cat, c_df in page_table.groupby(['is_text']):\n",
    "    ax1.hist(c_df['intensity'], \n",
    "             np.arange(255), \n",
    "             label = 'Text: {}'.format(c_cat), \n",
    "             alpha = 0.5) \n",
    "ax1.set_yscale(\"log\", nonposy='clip')\n",
    "ax1.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## What does the ROC curve look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "fpr, tpr, _ = roc_curve(page_table['is_text'], page_table['intensity'])\n",
    "roc_auc = roc_auc_score(page_table['is_text'], page_table['intensity'])\n",
    "fig, ax = plt.subplots(1,1)\n",
    "ax.plot(fpr, tpr, label='ROC curve (area = {0:0.2})'.format(roc_auc))\n",
    "ax.plot([0, 1], [0, 1], 'k--',label='Random guess')\n",
    "ax.set_xlim([0.0, 1.0]), ax.set_ylim([0.0, 1.05])\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.set_title('Receiver operating characteristic example')\n",
    "ax.legend(loc=\"lower right\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Adding Information\n",
    "Here we can improve the results by adding information. As we discussed in the second lecture on enhancement, edge-enhancing filters can be very useful for classifying images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def dog_filter(in_img, sig_1, sig_2):\n",
    "    return gaussian(page_image, sig_1) - gaussian(page_image, sig_2)\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize = (8, 4), dpi = 150)\n",
    "page_edges = dog_filter(page_image, 0.5, 10)\n",
    "ax1.imshow(page_image, cmap = 'bone')\n",
    "ax2.imshow(page_edges, cmap = 'bone')\n",
    "page_table['edges'] = page_edges.ravel()\n",
    "page_table.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(1,1)\n",
    "for c_cat, c_df in page_table.groupby(['is_text']):\n",
    "    ax1.hist(c_df['edges'], \n",
    "             label = 'Text: {}'.format(c_cat), \n",
    "             alpha = 0.5, bins=100)\n",
    "ax1.set_yscale(\"log\", nonposy='clip')\n",
    "ax1.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score \n",
    "fpr2, tpr2, _ = roc_curve(page_table['is_text'], \n",
    "                          page_table['intensity']/1000.0+page_table['edges'])\n",
    "roc_auc2 = roc_auc_score(page_table['is_text'], \n",
    "                         page_table['intensity']/1000.0+page_table['edges'])\n",
    "fig, ax = plt.subplots(1,1)\n",
    "ax.plot(fpr, tpr, label='Intensity curve (area = %0.2f)' % roc_auc)\n",
    "ax.plot(fpr2, tpr2, label='Combined curve (area = %0.2f)' % roc_auc2)\n",
    "ax.plot([0, 1], [0, 1], 'k--')\n",
    "ax.set_xlim([0.0, 1.0])\n",
    "ax.set_ylim([0.0, 1.05])    \n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.set_title('Receiver operating characteristic example')\n",
    "ax.legend(loc=\"lower right\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Why does the second filter perform better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize = (15, 5))\n",
    "for c_cat, c_df in page_table.groupby(['is_text']):\n",
    "    ax2.hist(c_df['edges'], \n",
    "             label = 'Text: {}'.format(c_cat), \n",
    "             alpha = 0.5, bins=100)\n",
    "ax2.set_yscale(\"log\", nonposy='clip')\n",
    "ax2.legend();\n",
    "\n",
    "for c_cat, c_df in page_table.groupby(['is_text']):\n",
    "    ax1.hist(c_df['intensity'], \n",
    "             np.arange(255), \n",
    "             label = 'Text: {}'.format(c_cat), \n",
    "             alpha = 0.5) \n",
    "ax1.set_yscale(\"log\", nonposy='clip')\n",
    "ax1.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# K-Means Clustering / Classification (Unsupervised)\n",
    "\n",
    "- Automatic clustering of multidimensional data into groups based on a distance metric\n",
    "- Fast and scalable to petabytes of data (Google, Facebook, Twitter, etc. use it regularly to classify customers, advertisements, queries)\n",
    "- __Input__ = feature vectors, distance metric, number of groups\n",
    "- __Output__ = a classification for each feature vector to a group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Example\n",
    "- Distance metric \n",
    "$$ D_{ij}=||\\vec{v}_i-\\vec{v}_j|| $$\n",
    "\n",
    "- Group Count ($N=2$)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "%matplotlib inline\n",
    "\n",
    "test_pts = pd.DataFrame(make_blobs(n_samples=200, random_state=2018)[\n",
    "                        0], columns=['x', 'y'])\n",
    "plt.plot(test_pts.x, test_pts.y, 'r.')\n",
    "test_pts.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "km = KMeans(n_clusters=2, random_state=2018)\n",
    "n_grp = km.fit_predict(test_pts)\n",
    "plt.scatter(test_pts.x, test_pts.y, c=n_grp)\n",
    "grp_pts = test_pts.copy()\n",
    "grp_pts['group'] = n_grp\n",
    "grp_pts.groupby(['group']).apply(lambda x: x.sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# K-Means Algorithm\n",
    "\n",
    "We give as an initial parameter the number of groups we want to find and possible a criteria for removing groups that are too similar\n",
    "\n",
    "1. Randomly create center points (groups) in vector space\n",
    "1. Assigns group to data point by the “closest” center\n",
    "1. Recalculate centers from mean point in each group\n",
    "1. Go back to step 2 until the groups stop changing\n",
    "\n",
    "*** \n",
    "\n",
    "What vector space do we have?\n",
    "- Sometimes represent physical locations (classify swiss people into cities)\n",
    "- Can include intensity or color (K-means can be used as a thresholding technique when you give it image intensity as the vector and tell it to find two or more groups)\n",
    "- Can also include orientation, shape, or in extreme cases full spectra (chemically sensitive imaging)\n",
    "\n",
    "#### Note:  If you look for N groups you will almost always find N groups with K-Means, whether or not they make any sense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "km = KMeans(n_clusters=4, random_state=2018)\n",
    "n_grp = km.fit_predict(test_pts)\n",
    "plt.scatter(test_pts.x, test_pts.y, c=n_grp)\n",
    "grp_pts = test_pts.copy()\n",
    "grp_pts['group'] = n_grp\n",
    "grp_pts.groupby(['group']).apply(lambda x: x.sample(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# K-Means Applied to Cortex Image\n",
    "\n",
    "In this example we use position and intensity as feature vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.io import imread\n",
    "%matplotlib inline\n",
    "cortex_img = imread(\"ext-figures/cortex.png\")[::3, ::3]/1000.0\n",
    "np.random.seed(2018)\n",
    "fig, (ax1) = plt.subplots(1, 1,\n",
    "                          figsize=(8, 8), dpi=72)\n",
    "ax1.imshow(cortex_img, cmap='bone')\n",
    "xx, yy = np.meshgrid(np.arange(cortex_img.shape[1]),\n",
    "                     np.arange(cortex_img.shape[0]))\n",
    "cortex_df = pd.DataFrame(dict(x=xx.ravel(),\n",
    "                              y=yy.ravel(),\n",
    "                              intensity=cortex_img.ravel()))\n",
    "cortex_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## First segmentation attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "km = KMeans(n_clusters=4, random_state=2018)\n",
    "cortex_df['group'] = km.fit_predict(cortex_df[['x', 'y', 'intensity']].values)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2,\n",
    "                               figsize=(12, 8), dpi=72)\n",
    "ax1.imshow(cortex_img, cmap='bone')\n",
    "ax2.imshow(cortex_df['group'].values.reshape(\n",
    "    cortex_img.shape), cmap='gist_earth')\n",
    "cortex_df.groupby(['group']).apply(lambda x: x.sample(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Why is the image segmented like this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# Rescaling components \n",
    "\n",
    "Since the distance is currently calculated by $||\\vec{v}_i-\\vec{v}_j||$ and the values for the position is much larger than the values for the _Intensity_ , _Sobel_ or _Gaussian_ they need to be rescaled so they all fit on the same axis \n",
    "$$\\vec{v} = \\left\\{\\frac{x}{10}, \\frac{y}{10}, \\textrm{Intensity}\\right\\}$$\n",
    "\n",
    "$N_{clusters}$=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters=4, random_state=2018)\n",
    "scale_cortex_df = cortex_df.copy()\n",
    "scale_cortex_df.x = scale_cortex_df.x/10\n",
    "scale_cortex_df.y = scale_cortex_df.y/10\n",
    "scale_cortex_df['group'] = km.fit_predict(\n",
    "    scale_cortex_df[['x', 'y', 'intensity']].values)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2,\n",
    "                               figsize=(12, 8), dpi=72)\n",
    "ax1.imshow(cortex_img, cmap='bone')\n",
    "ax2.imshow(scale_cortex_df['group'].values.reshape(cortex_img.shape),\n",
    "           cmap='gist_earth')\n",
    "scale_cortex_df.groupby(['group']).apply(lambda x: x.sample(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Let's try a different position scaling\n",
    "\n",
    "$$\\vec{v} = \\left\\{\\frac{x}{5}, \\frac{y}{5}, \\textrm{Intensity}\\right\\}$$\n",
    "$N_{clusters}$=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters=5, random_state=2019)\n",
    "scale_cortex_df = cortex_df.copy()\n",
    "scale_cortex_df.x = scale_cortex_df.x/5\n",
    "scale_cortex_df.y = scale_cortex_df.y/5\n",
    "scale_cortex_df['group'] = km.fit_predict(\n",
    "    scale_cortex_df[['x', 'y', 'intensity']].values)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2,\n",
    "                               figsize=(12, 8), dpi=72)\n",
    "ax1.imshow(cortex_img, cmap='bone')\n",
    "ax2.imshow(scale_cortex_df['group'].values.reshape(cortex_img.shape),\n",
    "           cmap='nipy_spectral')\n",
    "scale_cortex_df.groupby(['group']).apply(lambda x: x.sample(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Superpixels\n",
    "\n",
    "An approach for simplifying images by performing a clustering and forming super-pixels from groups of similar pixels. \n",
    "https://ivrl.epfl.ch/research/superpixels\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "<td><img src=\"https://www.epfl.ch/labs/ivrl/wp-content/uploads/2018/08/54082_combo.jpg\"/></td>\n",
    "<td><img src=\"https://www.epfl.ch/labs/ivrl/wp-content/uploads/2018/08/210088_combo.jpg\"/></td>\n",
    "<td><img src=\"https://www.epfl.ch/labs/ivrl/wp-content/uploads/2018/08/302003_combo.jpg\"/></td>\n",
    "        <tr>\n",
    "</table>\n",
    "\n",
    "[DOI](https://doi.org/10.1109/TPAMI.2012.120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Why use superpixels\n",
    "\n",
    "Super pixels\n",
    "- Drastically reduced data size, \n",
    "- Serves as an initial segmentation showing spatially meaningful groups\n",
    "\n",
    "---\n",
    "We start with an example of shale with multiple phases\n",
    "- rock\n",
    "- clay\n",
    "- pore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.io import imread\n",
    "%matplotlib inline\n",
    "shale_img = imread(\"../common/figures/shale-slice.tiff\")\n",
    "np.random.seed(2018)\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3,\n",
    "                                    figsize=(12, 4), dpi=100)\n",
    "ax1.imshow(shale_img, cmap='bone')\n",
    "thresh_vals = np.linspace(shale_img.min(), shale_img.max(), 5+2)[:-1]\n",
    "out_img = np.zeros_like(shale_img)\n",
    "for i, (t_start, t_end) in enumerate(zip(thresh_vals, thresh_vals[1:])):\n",
    "    thresh_reg = (shale_img > t_start) & (shale_img < t_end)\n",
    "    ax2.hist(shale_img.ravel()[thresh_reg.ravel()])\n",
    "    out_img[thresh_reg] = i\n",
    "ax3.imshow(out_img, cmap='gist_earth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from skimage.segmentation import slic, mark_boundaries\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(12, 4), dpi=100)\n",
    "shale_segs = slic(shale_img,\n",
    "                  n_segments=100,\n",
    "                  compactness=5e-2,\n",
    "                  sigma=3.0)\n",
    "ax1.imshow(shale_img, cmap='bone')\n",
    "ax1.set_title('Original Image')\n",
    "ax2.imshow(shale_segs, cmap='gist_earth')\n",
    "ax2.set_title('Superpixels')\n",
    "ax3.imshow(mark_boundaries(shale_img, shale_segs))\n",
    "ax3.set_title('Superpixel Overlay')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "flat_shale_img = shale_img.copy()\n",
    "for s_idx in np.unique(shale_segs.ravel()):\n",
    "    flat_shale_img[shale_segs == s_idx] = np.mean(\n",
    "        flat_shale_img[shale_segs == s_idx])\n",
    "fig, (ax1,ax2,ax3) = plt.subplots(1, 3, figsize=(15, 5))\n",
    "ax1.imshow(mark_boundaries(shale_img, shale_segs))\n",
    "ax1.set_title('Superpixel Overlay')\n",
    "ax2.imshow(flat_shale_img, cmap='bone'); ax2.set_title('Merged super pixels')\n",
    "ax3.imshow(flat_shale_img, cmap='Paired'); ax3.set_title('Merged super pixels (alternative color)');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3,\n",
    "                                    figsize=(12, 4), dpi=100)\n",
    "ax1.imshow(shale_img, cmap='viridis')\n",
    "thresh_vals = np.linspace(flat_shale_img.min(), flat_shale_img.max(), 5+2)[:-1]\n",
    "sp_out_img = np.zeros_like(flat_shale_img)\n",
    "for i, (t_start, t_end) in enumerate(zip(thresh_vals, thresh_vals[1:])):\n",
    "    thresh_reg = (flat_shale_img > t_start) & (flat_shale_img < t_end)\n",
    "    sp_out_img[thresh_reg] = i\n",
    "ax2.imshow(out_img, cmap='tab10')\n",
    "ax2 .set_title('Pixel Segmentation')\n",
    "ax3.imshow(sp_out_img, cmap='tab10')\n",
    "ax3.set_title('Superpixel Segmentation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Probabilistic Models of Segmentation\n",
    "\n",
    "A more general approach is to use a probabilistic model to segmentation. We start with our image $I(\\vec{x}) \\forall \\vec{x}\\in \\mathbb{R}^N$ and we classify it into two phases $\\alpha$ and $\\beta$\n",
    "\n",
    "$$P(\\{\\vec{x} , I(\\vec{x})\\}  | \\alpha) \\propto P(\\alpha) + P(I(\\vec{x}) | \\alpha)+  P(\\sum_{x^{\\prime} \\in \\mathcal{N}} I(\\vec{x^{\\prime}}) |  \\alpha)$$\n",
    "- $P(\\{\\vec{x} , f(\\vec{x})\\}  | \\alpha)$ the probability a given pixel is in phase $\\alpha$ given we know it's position and value (what we are trying to estimate)\n",
    "- $P(\\alpha)$ probability of any pixel in an image being part of the phase (expected volume fraction of that phase)\n",
    "- $P(I(\\vec{x}) | \\alpha)$ probability adjustment based on knowing the value of $I$ at the given point (standard threshold)\n",
    "- $P(f(\\vec{x^{\\prime}}) |  \\alpha)$ are the collective probability adjustments based on knowing the value of a pixels neighbors (very simple version of [Markov Random Field](http://en.wikipedia.org/wiki/Markov_random_field) approaches)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "livereveal": {
   "autolaunch": true,
   "enable_chalkboard": true,
   "footer": "March 19, 2020 - ETH 227-0966-00L: Quantitative Big Imaging/Advanced segmentation",
   "header": "<table width='100%' style='margin: 0px;'><tr><td align='left'><img src='../common/figures/eth_logo_kurz_pos.svg' style='height:30px;'></td><td align='right'><img src='../common/figures/PSI-Logo.svg' style='height:50px;'></td></tr></table>",
   "scroll": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
